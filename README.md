# Fandom Span ID Retrieval

This README documents the full codebase workflow and outputs. The project is organized as three tasks:

- **Task-0: Data processing and indexing**
- **Task-1: Span identification**
- **Task-2: Retrieval (article-level + paragraph-level + rerank)**

It also explains the data files, CSV/JSONL schemas, model choices, and code flow.

---

## Quickstart

1. Set the domain once for retrieval pipelines:
  - Edit [configs/retrieval/both_levels.yaml](configs/retrieval/both_levels.yaml) and set `domain`.
2. Scrape and build processed data:
  ```bash
  python scripts/01_Data_processing/00_scrape_fandom.py
  python scripts/01_Data_processing/01_build_ground_truth.py
  ```
3. Run span identification (Task-1):
  ```bash
  python scripts/02_Span_Identification/01_run_span_id_grid.py
  ```
4. Run retrieval (Task-2, both levels + rerank + aggregation):
  ```bash
  python scripts/03_Article_Retrieval/run_both_levels.py
  ```

---

## Pipeline Diagram

```mermaid
flowchart TD
   A[Scrape HTML] --> B[Build Ground Truth]
   B --> C[Processed JSONL/CSV]

   C --> D1[Task-1 Span Identification]
   D1 --> D2[Span Metrics + all_experiments.csv]

   C --> E1[Task-2 Article Retrieval]
   E1 --> E2[Article FAISS Index (lead/full)]
   E1 --> E3[Article Rerank]

   C --> F1[Task-2 Paragraph Retrieval]
   F1 --> F2[Paragraph FAISS Index]
   F1 --> F3[Paragraph Rerank]

   E3 --> G[Aggregate Results]
   F3 --> G
   G --> H[stats/<domain>.json + article_retrieval_results.csv]
```

---

## Task-0: Data Processing and Indexing

### Purpose
Build clean datasets from raw Fandom HTML, then prepare indexing resources for retrieval.

### Main scripts
- **Scrape HTML**: [scripts/01_Data_processing/00_scrape_fandom.py](scripts/01_Data_processing/00_scrape_fandom.py)
- **Build ground truth**: [scripts/01_Data_processing/01_build_ground_truth.py](scripts/01_Data_processing/01_build_ground_truth.py)

### Code flow (Task-0)
1. Scraper reads `configs/scraping.yaml` and downloads HTML into:
   - `data/raw/fandom_html/<domain>/`
2. Ground truth builder parses HTML and writes processed artifacts into:
   - `data/processed/<domain>/`
3. It updates `stats/<domain>.json` with dataset counts.

### Key outputs and schemas

#### `pages_<domain>.jsonl`
Each line represents a full page.

Fields:
- `granularity`: `page`
- `article_id`: numeric page id from HTML
- `page_name`: wiki page name (underscored)
- `title`: display title
- `page_id`: internal generated id
- `page_text`: full page text
- `url`: page URL
- `source_path`: source HTML path
- `links`: list of link objects with `anchor_text`, offsets, `article_id_of_internal_link`

#### `paragraphs_<domain>.jsonl`
Each line is a paragraph span extracted from the page.

Fields:
- `granularity`: `paragraph`
- `article_id`, `page_name`, `title`
- `paragraph_id`: internal generated id
- `paragraph_index`: paragraph index within page
- `paragraph_text`: paragraph content
- `url`, `source_path`
- `links`: list of paragraph-relative link objects

#### `sentences_<domain>.jsonl`
Each line is a sentence span extracted from the page.

Fields:
- `granularity`: `sentence`
- `article_id`, `page_name`, `title`
- `sentence_id`, `sentence_index`, `sentence_text`
- `url`, `source_path`
- `links`: list of sentence-relative link objects

#### `paragraphs_<domain>.csv` / `sentences_<domain>.csv`
Flat tables for quick loading. Fields are the same as in JSONL but without nested links.

#### `paragraph_links_<domain>.csv` / `sentence_links_<domain>.csv`
One row per link with paragraph- or sentence-relative offsets.

Columns:
- `paragraph_id` or `sentence_id`
- `anchor_index`, `anchor_text`, `link_type`
- `link_rel_start`, `link_rel_end`
- `target_page_name`, `article_id_of_internal_link`, `resolved_url`

#### `articles_<domain>.jsonl`
Generated by ground truth builder and used by article-level retrieval.

Fields:
- `article_id`
- `title` (uses `page_name` when available)
- `lead_paragraph` (first paragraph span)
- `full_text` (full page text)

---

## Task-1: Span Identification

### Purpose
Given text with linked spans, predict which tokens correspond to anchor spans.

### Main script
- [scripts/02_Span_Identification/01_run_span_id_grid.py](scripts/02_Span_Identification/01_run_span_id_grid.py)

### Code flow (Task-1)
1. Uses processed paragraph/sentence data from Task-0.
2. Trains span identification model(s) and evaluates.
3. Writes per-experiment metrics and a consolidated CSV.

### Models used
From outputs and configs, experiments include:
- `bert-base-uncased` (page and paragraph variants)

### Output artifacts
- `outputs/span_id/*.json` (per-experiment metrics)
- `outputs/span_id/all_experiments.csv` (summary table)

`all_experiments.csv` columns include:
- `experiment_name`, `timestamp`, `domain`, `model`, `level`, `normalize_punctuation`
- `eval_f1`, `eval_precision`, `eval_recall`
- `span_f1`, `span_precision`, `span_recall`
- `token_f1`, `token_precision`, `token_recall`
- `exact_span_*`, `relaxed_span_*`, `eval_loss`
- `num_epochs`, `learning_rate`, `batch_size`

---

## Task-2: Retrieval (Article + Paragraph)

This task has two retrieval levels and optional reranking.

### Paragraph-level retrieval
- Index unit: paragraph embeddings
- Output: ranked paragraphs mapped to article IDs
- Metrics: recall@k over target article ID

### Article-level retrieval
- Index unit: one vector per article
- Output: ranked article IDs
- Metrics: recall@k over target article ID

### Reranking
- Cross-encoder reranks candidates from retrieval stage
- Evaluated as recall@k after rerank

---

## Retrieval Models and Variants

### Paragraph-level
Current config ([configs/retrieval/paragraph_faiss.yaml](configs/retrieval/paragraph_faiss.yaml)):
- Embedding model: `sentence-transformers/all-MiniLM-L6-v2`
- Text variants: `paragraph_text`, `paragraph_text+title`
- Reranker: `cross-encoder/ms-marco-MiniLM-L-6-v2`

Historical outputs also include:
- `sentence-transformers/all-MiniLM-L12-v2`
- `Alibaba-NLP/gte-multilingual-base`

### Article-level
Current config ([configs/retrieval/experiment.yaml](configs/retrieval/experiment.yaml)):
- Encoder: `sentence-transformers/all-MiniLM-L6-v2`
- FAISS text variants: `lead_paragraph`, `full_text`

Reranker config ([configs/rerank/experiment.yaml](configs/rerank/experiment.yaml)):
- Cross-encoder: `cross-encoder/ms-marco-MiniLM-L-6-v2`
- Variants: `lead_paragraph`, `full_text`

---

## Retrieval Code Flow (Detailed)

### Paragraph-level pipeline
Entrypoint: [scripts/03_Article_Retrieval/run_paragraph_retrieval.py](scripts/03_Article_Retrieval/run_paragraph_retrieval.py)

Steps:
1. **Generate queries** from `paragraph_links_<domain>.csv` and context window.
2. **Build embeddings** for each paragraph variant.
3. **Build FAISS index** for each variant.
4. **Evaluate retrieval** by mapping retrieved paragraph IDs to article IDs.
5. **Prepare rerank data** by labeling top-k paragraph candidates.
6. **Train + evaluate reranker** with cross-encoder.
7. **Aggregate results** into CSV and `stats/<domain>.json`.

### Article-level pipeline
Entrypoint: [scripts/03_Article_Retrieval/run_both_levels.py](scripts/03_Article_Retrieval/run_both_levels.py)

Steps:
1. **Prepare retrieval data** (copies `articles_<domain>.jsonl` to `articles.jsonl`, splits queries).
2. **Train retrieval model** (article classifier).
3. **Evaluate retrieval model** (`test_metrics.json`).
4. **Build FAISS index** over article text variants.
5. **Prepare rerank data** from top-k retriever outputs.
6. **Train + evaluate reranker** per article text variant.
7. **Aggregate results** into CSV and `stats/<domain>.json`.

### Combined pipeline runner
Use the combined runner to control both levels from a single config:

- Config: [configs/retrieval/both_levels.yaml](configs/retrieval/both_levels.yaml)
- Runner: [scripts/03_Article_Retrieval/run_both_levels.py](scripts/03_Article_Retrieval/run_both_levels.py)

Flags in `both_levels.yaml`:
- `do_article`, `do_paragraph`
- `do_article_rerank`, `do_paragraph_rerank`
- `do_aggregate`

The domain is set in one place (`both_levels.yaml`) and overrides all sub-configs.

---

## Retrieval Outputs and File Schemas

### Queries
- `data/processed/<domain>/article_queries_<domain>.jsonl`
  - query_text + target article id for article-level retrieval
- `data/processed/<domain>/paragraph_queries_<domain>.jsonl`
  - query_text + anchor + source paragraph id + target article id

### Paragraph retrieval outputs
- `outputs/retrieval/<domain>/paragraph_embeddings_*`
- `outputs/retrieval/<domain>/paragraph_faiss_*`
- `outputs/retrieval/<domain>/paragraph_eval_*/<model>/<variant>/metrics.json`
  - `recall@1`, `recall@5`, `recall@10`

### Article retrieval outputs
- `outputs/retrieval/<domain>/<run_name>/test_metrics.json`
  - `recall@1`, `recall@5`, `recall@10`
- `outputs/retrieval/<domain>/faiss/<variant>/articles.faiss`
- `outputs/retrieval/<domain>/faiss/<variant>/article_embeddings.npy`
- `outputs/retrieval/<domain>/faiss/<variant>/article_ids.json`

### Rerank outputs
- Paragraph rerank data:
  - `outputs/rerank/<domain>/paragraph_*/*.jsonl`
- Paragraph rerank metrics:
  - `outputs/rerank/<domain>/paragraph_*/**/test_metrics.json`

- Article rerank data:
  - `data/processed/<domain>/rerank/<variant>/{train,val,test}.jsonl`
- Article rerank metrics:
  - `outputs/rerank/<domain>/<variant>/test_metrics.json`

### Aggregated results
- `outputs/rerank/article_retrieval_results.csv`
  - One row per experiment (article-level or rerank). Columns include:
    - `timestamp`, `task`, `domain`, `model`, `variant`, `seed`, `output_dir`
    - `recall@1`, `recall@5`, `recall@10`

### Stats JSON
- `stats/<domain>.json`
  - `dataset_stats`: counts from Task-0
  - `span_identification_metrics`: reserved
  - `retrieval_metrics`: latest article and rerank results
  - `experiments`: tracked experiment entries (if added)

---

## How to Run

### Task-0
```bash
python scripts/01_Data_processing/00_scrape_fandom.py
python scripts/01_Data_processing/01_build_ground_truth.py
```

### Task-1
```bash
python scripts/02_Span_Identification/01_run_span_id_grid.py
```

### Task-2 (combined)
```bash
python scripts/03_Article_Retrieval/run_both_levels.py
```

---

## Notes and Assumptions

- Ground truth uses HTML page structure; missing or malformed pages will be skipped.
- Retrieval evaluation is **article-level**, even in paragraph retrieval (paragraphs map back to article IDs).
- More text variants increase compute and outputs; limit variants if you need faster runs.
- The combined pipeline uses config-only toggles; you should not need to edit code to enable/disable components.
